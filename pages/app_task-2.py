import numpy as np
import pandas as pd
import re
import string
from time import time
from functools import partial
from pathlib import Path

import torch
from torch import nn
from torch.utils.data import DataLoader
import torch.nn.functional as F
from torchmetrics import F1Score

# –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã
import transformers
from transformers import AutoTokenizer, AutoModel
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import BertTokenizer, BertForSequenceClassification

import streamlit as st
import warnings
warnings.filterwarnings('ignore')

import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# –∑–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –∏  –º–æ–¥–µ–ª–∏, –ø–µ—Ä–µ–≤–æ–¥ –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–∂–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
script_dir = Path(__file__).parent.resolve()
PATH = script_dir.parent / 'models' / 'model_rtt.pth'
model_full = torch.load(PATH)
model_checkpoint = 'cointegrated/rubert-tiny-toxicity'
tokenizer_rtt = AutoTokenizer.from_pretrained(model_checkpoint)
model_full.eval()

# –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
def text2toxicity(text, model=model_full):
    probably_box = []
    with torch.no_grad():
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –∏ –ø–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        inputs = tokenizer_rtt(text, return_tensors='pt', truncation=True, padding=True).to(device)
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –∏ –ø–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        proba = torch.sigmoid(model(**inputs).logits).squeeze(1).cpu().numpy()
    for p in proba:
        if p > 0.5:
            probably_box.append(f'–û—Å–∫–æ—Ä–±–∏—Ç–µ–ª—å–Ω—ã–π, p={p:.2f}')
        else:
            probably_box.append(f'–ù–µ –æ—Å–∫–æ—Ä–±–∏—Ç–µ–ª—å–Ω—ã–π, p={p:.2f}')
    return pd.DataFrame({'–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç': text, 
                         '–ü—Ä–æ–≥–Ω–æ–∑': probably_box})


st.title("ü¶ú–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ —á–∞—Ç–æ–≤")
st.header('–†–∞—Å—á–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞')

txt_box = st.text_input("–í—Å—Ç–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç –≤ —Ä–∞–º–∫—É –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏")
if txt_box:
    st.write(text2toxicity(txt_box))

uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª –≤ —Ñ–æ—Ä–º–∞—Ç–µ *.txt", type='txt')
if uploaded_file is not None:
    lw = pd.read_csv(uploaded_file, header=None)[0].tolist()
    st.write(text2toxicity(lw))

st.header('–û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ –º–æ–¥–µ–ª–∏')
st.write('**–ó–∞–¥–∞—á–∞:** –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —Å–ø–æ—Å–æ–±–Ω—É—é —Ä–∞—Å–ø–æ–∑–Ω–æ–≤–∞—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏–∑ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π –æ—Å–∫–æ—Ä–±–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–ª–∏ —Ç–æ–∫—Å–∏—á–Ω–æ–≥–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∞')
st.write('**–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏:** rubert-tiny-toxicity')
st.write('**–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏:** 11.8M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤')
st.write('**–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è**: 14412 —Å—Ç—Ä–æ–∫')
st.write('**–û –¥–∞—Ç–∞—Å–µ—Ç–µ**: —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏–∑ —Ä–æ—Å—Å–∏–π—Å–∫–æ–π —Å–æ—Ü–∏–∞–ª—å–Ω–æ–π —Å–µ—Ç–∏ ok.ru')
st.write('**–õ—É—á—à–∏–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å loss train / valid**: 0.271 / 0.280')
st.write('**–õ—É—á—à–∏–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å accuracy train / valid**: 0.898 / 0.888')
st.write('**–õ—É—á—à–∏–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å f1_score**: 0.842')


st.header('–ì—Ä–∞—Ñ–∏–∫ –º–µ—Ç—Ä–∏–∫ –º–æ–¥–µ–ª–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è')

PATH_M= script_dir.parent / 'models' / 'rtt_metrics.json'
df_metrics = pd.read_json(PATH_M)
st.line_chart(df_metrics, x='epochs', x_label='—ç–ø–æ—Ö–∏', height=600)

st.header('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∏ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
PATH_I= script_dir.parent / 'images' / 'c_matrix_rtt.png'
st.image(PATH_I, caption=None)
